from copy import deepcopy
from sklearn.metrics import log_loss, accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. ê¹ƒí—ˆë¸Œì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ==========================================
url = "https://raw.githubusercontent.com/machine-learning-smart-fan/datagathering/main/newcode"
data_df = pd.read_csv(url, header=None, names=['current', 'level'])

# ë°ì´í„° í™•ì¸
print("=" * 60)
print("ğŸ“Š ì „ì²´ ë°ì´í„° ì •ë³´")
print("=" * 60)
print(f"ì „ì²´ ë°ì´í„° ê°œìˆ˜: {len(data_df)}")
print(f"\nê° ë ˆë²¨ë³„ ë°ì´í„° ê°œìˆ˜:")
print(data_df['level'].value_counts().sort_index())
print("\nê° ë ˆë²¨ë³„ ì „ë¥˜ í‰ê· :")
for level in sorted(data_df['level'].unique()):
    mean_current = data_df[data_df['level'] == level]['current'].mean()
    std_current = data_df[data_df['level'] == level]['current'].std()
    print(f"Level {level}: í‰ê·  {mean_current:.2f} mA (í‘œì¤€í¸ì°¨: {std_current:.2f})")

# ==========================================
# 2. ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ 80%, í…ŒìŠ¤íŠ¸ 20%)
# ==========================================
X = data_df[['current']].values
y = data_df['level'].values

# 80% í•™ìŠµ, 20% í…ŒìŠ¤íŠ¸ (stratifyë¡œ ê° ë ˆë²¨ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë¦¬)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\n" + "=" * 60)
print("ğŸ”€ ë°ì´í„° ë¶„í•  ê²°ê³¼")
print("=" * 60)
print(f"í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ (80%)")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ (20%)")
print(f"\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆë²¨ë³„ ê°œìˆ˜:")
unique, counts = np.unique(y_test, return_counts=True)
for level, count in zip(unique, counts):
    print(f"  Level {level}: {count}ê°œ")

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==========================================
# 3. SGD ë¶„ë¥˜ê¸° í•™ìŠµ
# ==========================================
sgd_clf = SGDClassifier(
    loss='log_loss', 
    penalty=None, 
    eta0=0.002, 
    learning_rate='constant',
    random_state=42
)

n_epochs = 400
best_valid_loss = float('inf')
train_errors, test_errors = [], []
best_model = None
best_epoch = 0

classes = np.unique(y)

print("\n" + "=" * 60)
print("ğŸ“ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("=" * 60)

for epoch in range(n_epochs):
    sgd_clf.partial_fit(X_train_scaled, y_train, classes=classes)
    
    y_train_proba = sgd_clf.predict_proba(X_train_scaled)
    train_error = log_loss(y_train, y_train_proba)
    train_errors.append(train_error)
    
    y_test_proba = sgd_clf.predict_proba(X_test_scaled)
    test_error = log_loss(y_test, y_test_proba)
    test_errors.append(test_error)
    
    if test_error < best_valid_loss:
        best_valid_loss = test_error
        best_model = deepcopy(sgd_clf)
        best_epoch = epoch
    
    if (epoch + 1) % 100 == 0:
        print(f"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_error:.4f}, Test Loss: {test_error:.4f}")

print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! Best Epoch: {best_epoch}")

# ==========================================
# 4. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ë° ê²°ê³¼ ë¶„ì„
# ==========================================
y_test_pred = best_model.predict(X_test_scaled)

# ë§ì¶˜ ê°œìˆ˜, í‹€ë¦° ê°œìˆ˜ ê³„ì‚°
correct_count = np.sum(y_test == y_test_pred)
wrong_count = np.sum(y_test != y_test_pred)
accuracy = correct_count / len(y_test) * 100

print("\n" + "=" * 60)
print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ({len(y_test)}ê°œ í…ŒìŠ¤íŠ¸)")
print("=" * 60)
print(f"âœ… ë§ì¶˜ ê°œìˆ˜: {correct_count}ê°œ")
print(f"âŒ í‹€ë¦° ê°œìˆ˜: {wrong_count}ê°œ")
print(f"ğŸ“Š ì •í™•ë„: {accuracy:.2f}%")

# ==========================================
# 5. ë ˆë²¨ë³„ ìƒì„¸ ê²°ê³¼
# ==========================================
print("\n" + "=" * 60)
print("ğŸ“‹ ë ˆë²¨ë³„ ìƒì„¸ ê²°ê³¼")
print("=" * 60)

level_names = {0: 'ì •ì§€(0ë‹¨)', 1: '1ë‹¨', 2: '2ë‹¨', 3: '3ë‹¨'}

for level in sorted(np.unique(y_test)):
    mask = y_test == level
    level_total = np.sum(mask)
    level_correct = np.sum((y_test == y_test_pred) & mask)
    level_wrong = level_total - level_correct
    level_accuracy = (level_correct / level_total * 100) if level_total > 0 else 0
    
    print(f"\n{level_names[level]}:")
    print(f"  ì „ì²´: {level_total}ê°œ")
    print(f"  ë§ì¶¤: {level_correct}ê°œ")
    print(f"  í‹€ë¦¼: {level_wrong}ê°œ")
    print(f"  ì •í™•ë„: {level_accuracy:.2f}%")

# ==========================================
# 6. í‹€ë¦° ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„
# ==========================================
wrong_indices = np.where(y_test != y_test_pred)[0]

if len(wrong_indices) > 0:
    print("\n" + "=" * 60)
    print(f"âŒ í‹€ë¦° ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„ (ì´ {len(wrong_indices)}ê°œ)")
    print("=" * 60)
    
    # í‹€ë¦° ì˜ˆì¸¡ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
    display_count = min(10, len(wrong_indices))
    for i, idx in enumerate(wrong_indices[:display_count]):
        actual_level = y_test[idx]
        predicted_level = y_test_pred[idx]
        current_value = X_test[idx][0]
        
        print(f"\ní‹€ë¦° ì˜ˆì¸¡ #{i+1}:")
        print(f"  ì „ë¥˜ê°’: {current_value:.2f} mA")
        print(f"  ì‹¤ì œ ë ˆë²¨: {level_names[actual_level]}")
        print(f"  ì˜ˆì¸¡ ë ˆë²¨: {level_names[predicted_level]}")
    
    if len(wrong_indices) > 10:
        print(f"\n... ì™¸ {len(wrong_indices) - 10}ê°œ ë” ìˆìŒ")
else:
    print("\n" + "=" * 60)
    print("ğŸ‰ ì™„ë²½í•©ë‹ˆë‹¤! ëª¨ë“  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤!")
    print("=" * 60)

# ==========================================
# 7. Confusion Matrix
# ==========================================
cm = confusion_matrix(y_test, y_test_pred)
print("\n" + "=" * 60)
print("ğŸ“Š Confusion Matrix (í˜¼ë™ í–‰ë ¬)")
print("=" * 60)
print("           ì˜ˆì¸¡â†’")
print("ì‹¤ì œâ†“     0ë‹¨    1ë‹¨    2ë‹¨    3ë‹¨")
for i, level in enumerate(sorted(np.unique(y_test))):
    row_str = f"{level_names[level]:8} "
    for j in range(len(cm[i])):
        row_str += f"{cm[i][j]:6} "
    print(row_str)

# ==========================================
# 8. ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ ìµœì¢… ë¶„ë¥˜ ê³µì‹ ì¶œë ¥
# ==========================================
print("\n" + "=" * 60)
print("âš™ï¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ ë¶„ë¥˜ ê³µì‹")
print("=" * 60)

# ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
print("\n[1] StandardScaler íŒŒë¼ë¯¸í„°:")
print(f"  í‰ê·  (mean): {scaler.mean_[0]:.6f}")
print(f"  í‘œì¤€í¸ì°¨ (scale): {scaler.scale_[0]:.6f}")
print(f"\n  ìŠ¤ì¼€ì¼ë§ ê³µì‹: X_scaled = (X - {scaler.mean_[0]:.6f}) / {scaler.scale_[0]:.6f}")

# ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì ˆí¸
print("\n[2] SGD Classifier ê°€ì¤‘ì¹˜ì™€ ì ˆí¸:")
print(f"  ëª¨ë¸ í´ë˜ìŠ¤: {best_model.classes_}")

weights = best_model.coef_
intercepts = best_model.intercept_

for i, level in enumerate(best_model.classes_):
    print(f"\n  Level {level} ({level_names[level]}):")
    print(f"    ê°€ì¤‘ì¹˜ (w): {weights[i][0]:.6f}")
    print(f"    ì ˆí¸ (b): {intercepts[i]:.6f}")

# ìµœì¢… ë¶„ë¥˜ ê³µì‹
print("\n[3] ìµœì¢… ë¶„ë¥˜ ê³µì‹ (Logistic Regression - One-vs-Rest):")
print("\nê° í´ë˜ìŠ¤ì˜ ì ìˆ˜ ê³„ì‚°:")
for i, level in enumerate(best_model.classes_):
    w = weights[i][0]
    b = intercepts[i]
    mean = scaler.mean_[0]
    scale = scaler.scale_[0]
    
    print(f"\nLevel {level} ({level_names[level]}):")
    print(f"  score_{level} = w * X_scaled + b")
    print(f"  score_{level} = {w:.6f} * ((ì „ë¥˜ - {mean:.6f}) / {scale:.6f}) + {b:.6f}")
    
    # ê°„ì†Œí™”ëœ ê³µì‹
    final_w = w / scale
    final_b = b - (w * mean / scale)
    print(f"  âŸ¹ score_{level} = {final_w:.6f} * ì „ë¥˜ + {final_b:.6f}")

print("\nìµœì¢… ì˜ˆì¸¡:")
print("  ê° ë ˆë²¨ì˜ scoreë¥¼ ê³„ì‚°í•œ í›„,")
print("  softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³ ,")
print("  ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë ˆë²¨ì„ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")

print("\n  P(level_i) = exp(score_i) / Î£ exp(score_j)")
print("  predicted_level = argmax(P(level_0), P(level_1), P(level_2), P(level_3))")

# ==========================================
# 9. ì‹¤ì œ ì˜ˆì¸¡ ì˜ˆì‹œ
# ==========================================
print("\n" + "=" * 60)
print("ğŸ“ ì‹¤ì œ ì˜ˆì¸¡ ì˜ˆì‹œ")
print("=" * 60)

test_currents = [30, 75, 110, 156]

for test_current in test_currents:
    print(f"\nì „ë¥˜ = {test_current} mA ì¼ ë•Œ:")
    
    # ìŠ¤ì¼€ì¼ë§
    X_scaled = (test_current - scaler.mean_[0]) / scaler.scale_[0]
    print(f"  1. ìŠ¤ì¼€ì¼ë§: ({test_current} - {scaler.mean_[0]:.2f}) / {scaler.scale_[0]:.2f} = {X_scaled:.6f}")
    
    # ê° ë ˆë²¨ì˜ ì ìˆ˜ ê³„ì‚°
    print(f"  2. ê° ë ˆë²¨ì˜ ì ìˆ˜ ê³„ì‚°:")
    scores = []
    for i, level in enumerate(best_model.classes_):
        score = weights[i][0] * X_scaled + intercepts[i]
        scores.append(score)
        print(f"     Level {level}: {weights[i][0]:.6f} * {X_scaled:.6f} + {intercepts[i]:.6f} = {score:.6f}")
    
    # Softmax í™•ë¥  ê³„ì‚°
    exp_scores = np.exp(scores)
    probabilities = exp_scores / np.sum(exp_scores)
    
    print(f"  3. Softmax í™•ë¥ :")
    for i, level in enumerate(best_model.classes_):
        print(f"     Level {level} ({level_names[level]}): {probabilities[i]*100:.2f}%")
    
    # ìµœì¢… ì˜ˆì¸¡
    predicted_level = best_model.classes_[np.argmax(probabilities)]
    print(f"  4. ìµœì¢… ì˜ˆì¸¡: Level {predicted_level} ({level_names[predicted_level]})")

# ==========================================
# 10. ì‹œê°í™”
# ==========================================
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# (1) í•™ìŠµ ê³¡ì„ 
ax1 = axes[0, 0]
ax1.annotate('Best model',
             xy=(best_epoch, best_valid_loss),
             xytext=(best_epoch, best_valid_loss + 0.05),
             ha="center",
             fontsize=10,
             arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))
ax1.plot([0, len(test_errors)], [best_valid_loss, best_valid_loss], 
         "k:", linewidth=2, label=f'Best Test Loss: {best_valid_loss:.4f}')
ax1.plot(test_errors, "b-", linewidth=2, label="Test Loss")
ax1.plot(train_errors, "r--", linewidth=2, label="Training Loss")
ax1.plot(best_epoch, best_valid_loss, "bo", markersize=8)
ax1.legend(loc="upper right")
ax1.set_xlabel("Epoch")
ax1.set_ylabel("Log Loss")
ax1.set_title("Training and Test Loss")
ax1.grid(True, alpha=0.3)

# (2) ì „ë¥˜ ë¶„í¬ (í•™ìŠµ ë°ì´í„°)
ax2 = axes[0, 1]
for level in sorted(np.unique(y_train)):
    level_data = X_train[y_train == level].flatten()
    ax2.hist(level_data, bins=30, alpha=0.6, label=f'Level {level}')
ax2.set_xlabel("Current (mA)")
ax2.set_ylabel("Frequency")
ax2.set_title("Training Data: Current Distribution by Level")
ax2.legend()
ax2.grid(True, alpha=0.3)

# (3) Confusion Matrix íˆíŠ¸ë§µ
ax3 = axes[1, 0]
level_labels = [level_names[i] for i in sorted(np.unique(y_test))]
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=level_labels, yticklabels=level_labels, ax=ax3,
            cbar_kws={'label': 'Count'})
ax3.set_xlabel("Predicted Level")
ax3.set_ylabel("True Level")
ax3.set_title(f"Confusion Matrix\n(Accuracy: {accuracy:.2f}%)")

# (4) ê²°ì • ê²½ê³„ ì‹œê°í™”
ax4 = axes[1, 1]

# ì „ë¥˜ ë²”ìœ„ ì„¤ì •
current_range = np.linspace(0, 200, 1000).reshape(-1, 1)
current_scaled = scaler.transform(current_range)
probabilities = best_model.predict_proba(current_scaled)

for i, level in enumerate(best_model.classes_):
    ax4.plot(current_range, probabilities[:, i], label=f'Level {level} ({level_names[level]})', linewidth=2)

ax4.set_xlabel("Current (mA)")
ax4.set_ylabel("Probability")
ax4.set_title("Decision Boundaries (Probability)")
ax4.legend()
ax4.grid(True, alpha=0.3)
ax4.set_xlim([0, 200])
ax4.set_ylim([0, 1.05])

plt.tight_layout()
plt.show()

# ==========================================
# 11. ìµœì¢… ìš”ì•½
# ==========================================
print("\n" + "=" * 60)
print("ğŸ“ ìµœì¢… ìš”ì•½")
print("=" * 60)
print(f"ì´ ë°ì´í„°: {len(data_df)}ê°œ")
print(f"í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ (80%)")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ (20%)")
print(f"\ní…ŒìŠ¤íŠ¸ ê²°ê³¼:")
print(f"  âœ… ë§ì¶˜ ê°œìˆ˜: {correct_count}/{len(y_test)}ê°œ")
print(f"  âŒ í‹€ë¦° ê°œìˆ˜: {wrong_count}/{len(y_test)}ê°œ")
print(f"  ğŸ“Š ì •í™•ë„: {accuracy:.2f}%")
print("=" * 60)
